{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce for joining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do two examples regarding to joining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: (date word, day-count) + (word, total-count) --> (date word day-count total-count)\n",
    "E.g. data1 (able, 991) + data2 (Jan-01 able, 5) --> data3 (Jan-01 able 5 991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#This mapper code will input a <date word, value> input file, and move date into \n",
    "#  the value field for output\n",
    "#  \n",
    "#  Note, this program is written in a simple style and does not full advantage of Python \n",
    "#     data structures,but I believe it is more readable\n",
    "#\n",
    "#  Note, there is NO error checking of the input, it is assumed to be correct\n",
    "#     meaning no extra spaces, missing inputs or counts,etc..\n",
    "#\n",
    "# See #  see https://docs.python.org/2/tutorial/index.html for details  and python  tutorials\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line       = line.strip()   #strip out carriage return\n",
    "    key_value  = line.split(\",\")   #split line, into key and value, returns a list\n",
    "    key_in     = key_value[0].split(\" \")   #key is first item in list\n",
    "    value_in   = key_value[1]   #value is 2nd item \n",
    "\n",
    "    #print key_in\n",
    "    if len(key_in)>=2:           #if this entry has <date word> in key\n",
    "        date = key_in[0]      #now get date from key field\n",
    "        word = key_in[1]\n",
    "        value_out = date+\" \"+value_in     #concatenate date, blank, and value_in\n",
    "        print( '%s\\t%s' % (word, value_out) )  #print a string, tab, and string\n",
    "    else:   #key is only <word> so just pass it through\n",
    "        print( '%s\\t%s' % (key_in[0], value_in) )  #print a string tab and string\n",
    "\n",
    "#Note that Hadoop expects a tab to separate key value\n",
    "#but this program assumes the input file has a ',' separating key value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#This reducer code will input a <word, value> input file, and join words together\n",
    "# Note the input will come as a group of lines with same word (ie the key)\n",
    "# As it reads words it will hold on to the value field\n",
    "#\n",
    "# It will keep track of current word and previous word, if word changes\n",
    "#   then it will perform the 'join' on the set of held values by merely printing out \n",
    "#   the word and values.  In other words, there is no need to explicitly match keys b/c\n",
    "#   Hadoop has already put them sequentially in the input \n",
    "#   \n",
    "# At the end it will perform the last join\n",
    "#\n",
    "#\n",
    "#  Note, there is NO error checking of the input, it is assumed to be correct, meaning\n",
    "#   it has word with correct and matching entries, no extra spaces, etc.\n",
    "#\n",
    "#  see https://docs.python.org/2/tutorial/index.html for python tutorials\n",
    "#\n",
    "#  San Diego Supercomputer Center copyright\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "prev_word          = \"  \"                #initialize previous word  to blank string\n",
    "months             = ['Jan','Feb','Mar','Apr','Jun','Jul','Aug','Sep','Nov','Dec']\n",
    "\n",
    "dates_to_output    = [] #an empty list to hold dates for a given word\n",
    "day_cnts_to_output = [] #an empty list of day counts for a given word\n",
    "# see https://docs.python.org/2/tutorial/datastructures.html for list details\n",
    "\n",
    "line_cnt           = 0  #count input lines\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line       = line.strip()       #strip out carriage return\n",
    "    key_value  = line.split('\\t')   #split line, into key and value, returns a list\n",
    "    line_cnt   = line_cnt+1     \n",
    "\n",
    "    #note: for simple debugging use print statements, ie:  \n",
    "    curr_word  = key_value[0]         #key is first item in list, indexed by 0\n",
    "    value_in   = key_value[1]         #value is 2nd item\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Check if its a new word and not the first line \n",
    "    #   (b/c for the first line the previous word is not applicable)\n",
    "    #   if so then print out list of dates and counts\n",
    "    #----------------------------------------------------\n",
    "    if curr_word != prev_word:\n",
    "\n",
    "        # -----------------------     \n",
    "\t#now write out the join result, but not for the first line input\n",
    "        # -----------------------\n",
    "        if line_cnt>1:\n",
    "\t    for i in range(len(dates_to_output)):  #loop thru dates, indexes start at 0\n",
    "\t         print('{0} {1} {2} {3}'.format(dates_to_output[i],prev_word,day_cnts_to_output[i],curr_word_total_cnt))\n",
    "            #now reset lists\n",
    "\t    dates_to_output   =[]\n",
    "            day_cnts_to_output=[]\n",
    "        prev_word         =curr_word  #set up previous word for the next set of input lines\n",
    "\n",
    "\t\n",
    "    # ---------------------------------------------------------------\n",
    "    #whether or not the join result was written out, \n",
    "    #   now process the curr word    \n",
    "  \t\n",
    "    #determine if its from file <word, total-count> or < word, date day-count>\n",
    "    # and build up list of dates, day counts, and the 1 total count\n",
    "    # ---------------------------------------------------------------\n",
    "    if (value_in[0:3] in months): \n",
    "\n",
    "        date_day =value_in.split() #split the value field into a date and day-cnt\n",
    "        \n",
    "        #add date to lists of the value fields we are building\n",
    "        dates_to_output.append(date_day[0])\n",
    "        day_cnts_to_output.append(date_day[1])\n",
    "    else:\n",
    "        curr_word_total_cnt = value_in  #if the value field was just the total count then its\n",
    "                                           #the first (and only) item in this list\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#now write out the LAST join result\n",
    "# ---------------------------------------------------------------\n",
    "for i in range(len(dates_to_output)):  #loop thru dates, indexes start at 0\n",
    "         print('{0} {1} {2} {3}'.format(dates_to_output[i],prev_word,day_cnts_to_output[i],curr_word_total_cnt))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_FileA.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "able,991\n",
    "about,11\n",
    "burger,15\n",
    "actor,22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_FileB.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Jan-01 able,5\n",
    "Feb-02 about,3\n",
    "Mar-03 about,8\n",
    "Apr-04 able,13\n",
    "Feb-22 actor,3\n",
    "Feb-23 burger,5\n",
    "Mar-08 burger,2\n",
    "Dec-15 able,100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Copy and paste the above into a text file as follows from the terminal prompt in Cloudera VM.\n",
    "\n",
    "Type in the following to open a text editor, and then cut and paste the above lines for join1_mapper.py, join1_reducer.py, join1_FileA.txt, and join1_FileB.txt into the text editor, save, and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gedit join1_mapper.py\n",
    "gedit join1_reducer.py\n",
    "gedit join1_FileA.txt\n",
    "gedit join1_FileB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Enter the following to make functions executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x join1_mapper.py\n",
    "chmod +x join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Create a directory on the HDFS file system (if already exists thatâ€™s OK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /user/cloudera/input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: make sure the input folder only has the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)Copy the files from local filesystem to the HDFS filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -put /home/cloudera/join1_FileA.txt /user/cloudera/input1\n",
    "\n",
    "hdfs dfs -put /home/cloudera/join1_FileB.txt /user/cloudera/input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see your files on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/cloudera/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)Test the program in serial execution using the following Unix utilities and piping commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat join1_File*.txt | ./join1_mapper.py | sort | ./join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also only test mapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat join1_File*.txt | ./join1_mapper.py | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: `|` pipes the standard output to the standard input of the join_mapper program, etc..\n",
    "\n",
    "To debug programs in serial execution one should use small datasets and possibly extra print statements in the program. Debugging with map/reduce jobs is harder but hopefully not necessary for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)Run the Hadoop streaming command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input1 \\\n",
    "-output /user/cloudera/output_join \\   \n",
    "-mapper /home/cloudera/join1_mapper.py \\   \n",
    "-reducer /home/cloudera/join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: (TV show, count) + (TV show title, channel) --> (TV show, total-count) for ABC channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python   \n",
    "#the above just indicates to use python to intepret this file\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()   #strip out carriage return\n",
    "    key_value = line.split(\",\")   #split line, into key and value, returns a list\n",
    "    key_in = key_value[0]\n",
    "    value_in = key_value[1]\n",
    "    if value_in.isdigit():\n",
    "        print('{0}\\t{1}'.format(key_in, value_in) )\n",
    "    elif value_in == 'ABC':\n",
    "        print('{0}\\t{1}'.format(key_in, value_in) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python   \n",
    "#the above just indicates to use python to intepret this file\n",
    "import sys\n",
    "\n",
    "last_key      = None              #initialize these variables\n",
    "running_total = 0\n",
    "abc_found = False\n",
    "# -----------------------------------\n",
    "# Loop thru file\n",
    "#  --------------------------------\n",
    "for input_line in sys.stdin:\n",
    "    input_line = input_line.strip()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Get Next Word    # --------------------------------\n",
    "    this_key, value = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "                          #the split command returns a list of strings, in this case into 2 variables\n",
    "    \n",
    "        \n",
    "    if last_key == this_key:     #check if key has changed ('==' is                                   #      logical equalilty check\n",
    "        if value.isdigit():\n",
    "            value = int(value)\n",
    "            running_total += value \n",
    "        elif value == 'ABC':\n",
    "            abc_found = True\n",
    "    else:\n",
    "        if abc_found:         \n",
    "            print( \"{0}\\t{1}\".format(last_key, running_total) )\n",
    "            abc_found = False                    \n",
    "        if value.isdigit():\n",
    "            value = int(value)\n",
    "            running_total = value \n",
    "        elif value == 'ABC':\n",
    "            abc_found = True\n",
    "        last_key = this_key\n",
    "\n",
    "if last_key == this_key & abc_found:\n",
    "    print( \"{0}\\t{1}\".format(last_key, running_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_join2data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#  (make_join2data.py) Generate a random combination of titles and viewer counts, or channels\n",
    "# this is a simple version of a congruential generator, \n",
    "#   not a great random generator but enough  \n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "chans   = ['ABC','DEF','CNO','NOX','YES','CAB','BAT','MAN','ZOO','XYZ','BOB']\n",
    "sh1 =['Hot','Almost','Hourly','PostModern','Baked','Dumb','Cold','Surreal','Loud']\n",
    "sh2 =['News','Show','Cooking','Sports','Games','Talking','Talking']\n",
    "vwr =range(17,1053)\n",
    "\n",
    "chvnm=sys.argv[1]  #get number argument, if its n, do numbers not channels,\n",
    "\n",
    "lch=len(chans)\n",
    "lsh1=len(sh1)\n",
    "lsh2=len(sh2)\n",
    "lvwr=len(vwr)\n",
    "ci=1\n",
    "s1=2\n",
    "s2=3\n",
    "vwi=4\n",
    "ri=int(sys.argv[3])\n",
    "for i in range(0,int(sys.argv[2])):  #arg 2 is the number of lines to output\n",
    "\n",
    "    if chvnm=='n':  #no numuber\n",
    "        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],chans[ci]))\n",
    "    else:\n",
    "        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],vwr[vwi])) \n",
    "    ci=(5*ci+ri) % lch   \n",
    "    s1=(4*s1+ri) % lsh1\n",
    "    s2=(3*s1+ri+i) % lsh2\n",
    "    vwi=(2*vwi+ri+i) % lvwr\n",
    " \n",
    "    if (vwi==4): vwi=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_data_join2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python make_join2data.py y 1000 13 > join2_gennumA.txt\n",
    "python make_join2data.py y 2000 17 > join2_gennumB.txt\n",
    "python make_join2data.py y 3000 19 > join2_gennumC.txt\n",
    "python make_join2data.py n 100  23 > join2_genchanA.txt\n",
    "python make_join2data.py n 200  19 > join2_genchanB.txt\n",
    "python make_join2data.py n 300  37 > join2_genchanC.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is exacutly with Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Hadoop streaming command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input3 \\\n",
    "-output /user/cloudera/output_join2 \\   \n",
    "-mapper /home/cloudera/join2_mapper.py \\   \n",
    "-reducer /home/cloudera/join2_reducer.py \\\n",
    "-numReduceTasks 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
