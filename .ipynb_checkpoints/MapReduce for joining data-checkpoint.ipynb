{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce for joining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do two examples regarding to joining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: (date word, day-count) + (word, total-count) --> (date word day-count total-count)\n",
    "E.g. data1 (able, 991) + data2 (Jan-01 able, 5) --> data3 (Jan-01 able 5 991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#This mapper code will input a <date word, value> input file, and move date into \n",
    "#  the value field for output\n",
    "#  \n",
    "#  Note, this program is written in a simple style and does not full advantage of Python \n",
    "#     data structures,but I believe it is more readable\n",
    "#\n",
    "#  Note, there is NO error checking of the input, it is assumed to be correct\n",
    "#     meaning no extra spaces, missing inputs or counts,etc..\n",
    "#\n",
    "# See #  see https://docs.python.org/2/tutorial/index.html for details  and python  tutorials\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line       = line.strip()   #strip out carriage return\n",
    "    key_value  = line.split(\",\")   #split line, into key and value, returns a list\n",
    "    key_in     = key_value[0].split(\" \")   #key is first item in list\n",
    "    value_in   = key_value[1]   #value is 2nd item \n",
    "\n",
    "    #print key_in\n",
    "    if len(key_in)>=2:           #if this entry has <date word> in key\n",
    "        date = key_in[0]      #now get date from key field\n",
    "        word = key_in[1]\n",
    "        value_out = date+\" \"+value_in     #concatenate date, blank, and value_in\n",
    "        print( '%s\\t%s' % (word, value_out) )  #print a string, tab, and string\n",
    "    else:   #key is only <word> so just pass it through\n",
    "        print( '%s\\t%s' % (key_in[0], value_in) )  #print a string tab and string\n",
    "\n",
    "#Note that Hadoop expects a tab to separate key value\n",
    "#but this program assumes the input file has a ',' separating key value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#This reducer code will input a <word, value> input file, and join words together\n",
    "# Note the input will come as a group of lines with same word (ie the key)\n",
    "# As it reads words it will hold on to the value field\n",
    "#\n",
    "# It will keep track of current word and previous word, if word changes\n",
    "#   then it will perform the 'join' on the set of held values by merely printing out \n",
    "#   the word and values.  In other words, there is no need to explicitly match keys b/c\n",
    "#   Hadoop has already put them sequentially in the input \n",
    "#   \n",
    "# At the end it will perform the last join\n",
    "#\n",
    "#\n",
    "#  Note, there is NO error checking of the input, it is assumed to be correct, meaning\n",
    "#   it has word with correct and matching entries, no extra spaces, etc.\n",
    "#\n",
    "#  see https://docs.python.org/2/tutorial/index.html for python tutorials\n",
    "#\n",
    "#  San Diego Supercomputer Center copyright\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "prev_word          = \"  \"                #initialize previous word  to blank string\n",
    "months             = ['Jan','Feb','Mar','Apr','Jun','Jul','Aug','Sep','Nov','Dec']\n",
    "\n",
    "dates_to_output    = [] #an empty list to hold dates for a given word\n",
    "day_cnts_to_output = [] #an empty list of day counts for a given word\n",
    "# see https://docs.python.org/2/tutorial/datastructures.html for list details\n",
    "\n",
    "line_cnt           = 0  #count input lines\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line       = line.strip()       #strip out carriage return\n",
    "    key_value  = line.split('\\t')   #split line, into key and value, returns a list\n",
    "    line_cnt   = line_cnt+1     \n",
    "\n",
    "    #note: for simple debugging use print statements, ie:  \n",
    "    curr_word  = key_value[0]         #key is first item in list, indexed by 0\n",
    "    value_in   = key_value[1]         #value is 2nd item\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Check if its a new word and not the first line \n",
    "    #   (b/c for the first line the previous word is not applicable)\n",
    "    #   if so then print out list of dates and counts\n",
    "    #----------------------------------------------------\n",
    "    if curr_word != prev_word:\n",
    "\n",
    "        # -----------------------     \n",
    "\t#now write out the join result, but not for the first line input\n",
    "        # -----------------------\n",
    "        if line_cnt>1:\n",
    "\t    for i in range(len(dates_to_output)):  #loop thru dates, indexes start at 0\n",
    "\t         print('{0} {1} {2} {3}'.format(dates_to_output[i],prev_word,day_cnts_to_output[i],curr_word_total_cnt))\n",
    "            #now reset lists\n",
    "\t    dates_to_output   =[]\n",
    "            day_cnts_to_output=[]\n",
    "        prev_word         =curr_word  #set up previous word for the next set of input lines\n",
    "\n",
    "\t\n",
    "    # ---------------------------------------------------------------\n",
    "    #whether or not the join result was written out, \n",
    "    #   now process the curr word    \n",
    "  \t\n",
    "    #determine if its from file <word, total-count> or < word, date day-count>\n",
    "    # and build up list of dates, day counts, and the 1 total count\n",
    "    # ---------------------------------------------------------------\n",
    "    if (value_in[0:3] in months): \n",
    "\n",
    "        date_day =value_in.split() #split the value field into a date and day-cnt\n",
    "        \n",
    "        #add date to lists of the value fields we are building\n",
    "        dates_to_output.append(date_day[0])\n",
    "        day_cnts_to_output.append(date_day[1])\n",
    "    else:\n",
    "        curr_word_total_cnt = value_in  #if the value field was just the total count then its\n",
    "                                           #the first (and only) item in this list\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#now write out the LAST join result\n",
    "# ---------------------------------------------------------------\n",
    "for i in range(len(dates_to_output)):  #loop thru dates, indexes start at 0\n",
    "         print('{0} {1} {2} {3}'.format(dates_to_output[i],prev_word,day_cnts_to_output[i],curr_word_total_cnt))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_FileA.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "able,991\n",
    "about,11\n",
    "burger,15\n",
    "actor,22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join1_FileB.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Jan-01 able,5\n",
    "Feb-02 about,3\n",
    "Mar-03 about,8\n",
    "Apr-04 able,13\n",
    "Feb-22 actor,3\n",
    "Feb-23 burger,5\n",
    "Mar-08 burger,2\n",
    "Dec-15 able,100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Copy and paste the above into a text file as follows from the terminal prompt in Cloudera VM.\n",
    "\n",
    "Type in the following to open a text editor, and then cut and paste the above lines for join1_mapper.py, join1_reducer.py, join1_FileA.txt, and join1_FileB.txt into the text editor, save, and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gedit join1_mapper.py\n",
    "gedit join1_reducer.py\n",
    "gedit join1_FileA.txt\n",
    "gedit join1_FileB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Enter the following to make functions executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x join1_mapper.py\n",
    "chmod +x join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Create a directory on the HDFS file system (if already exists thatâ€™s OK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /user/cloudera/input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: make sure the input folder only has the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)Copy the files from local filesystem to the HDFS filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -put /home/cloudera/join1_FileA.txt /user/cloudera/input1\n",
    "\n",
    "hdfs dfs -put /home/cloudera/join1_FileB.txt /user/cloudera/input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see your files on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/cloudera/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)Test the program in serial execution using the following Unix utilities and piping commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat join1_File*.txt | ./join1_mapper.py | sort | ./join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also only test mapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat join1_File*.txt | ./join1_mapper.py | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: `|` pipes the standard output to the standard input of the join_mapper program, etc..\n",
    "\n",
    "To debug programs in serial execution one should use small datasets and possibly extra print statements in the program. Debugging with map/reduce jobs is harder but hopefully not necessary for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)Run the Hadoop streaming command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input1 \\\n",
    "-output /user/cloudera/output_join \\   \n",
    "-mapper /home/cloudera/join1_mapper.py \\   \n",
    "-reducer /home/cloudera/join1_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: (TV show, count) + (TV show title, channel) --> (TV show, total-count) for ABC channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python   \n",
    "#the above just indicates to use python to intepret this file\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()   #strip out carriage return\n",
    "    key_value = line.split(\",\")   #split line, into key and value, returns a list\n",
    "    key_in = key_value[0]\n",
    "    value_in = key_value[1]\n",
    "    if value_in.isdigit():\n",
    "        print('{0}\\t{1}'.format(key_in, value_in) )\n",
    "    elif value_in == 'ABC':\n",
    "        print('{0}\\t{1}'.format(key_in, value_in) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python   \n",
    "#the above just indicates to use python to intepret this file\n",
    "import sys\n",
    "\n",
    "last_key      = None              #initialize these variables\n",
    "running_total = 0\n",
    "abc_found = False\n",
    "# -----------------------------------\n",
    "# Loop thru file\n",
    "#  --------------------------------\n",
    "for input_line in sys.stdin:\n",
    "    input_line = input_line.strip()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Get Next Word    # --------------------------------\n",
    "    this_key, value = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "                          #the split command returns a list of strings, in this case into 2 variables\n",
    "    \n",
    "        \n",
    "    if last_key == this_key:     #check if key has changed ('==' is                                   #      logical equalilty check\n",
    "        if value.isdigit():\n",
    "            value = int(value)\n",
    "            running_total += value \n",
    "        elif value == 'ABC':\n",
    "            abc_found = True\n",
    "    else:\n",
    "        if abc_found:         \n",
    "            print( \"{0}\\t{1}\".format(last_key, running_total) )\n",
    "            abc_found = False                    \n",
    "        if value.isdigit():\n",
    "            value = int(value)\n",
    "            running_total = value \n",
    "        elif value == 'ABC':\n",
    "            abc_found = True\n",
    "        last_key = this_key\n",
    "\n",
    "if last_key == this_key & abc_found:\n",
    "    print( \"{0}\\t{1}\".format(last_key, running_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_join2data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#  (make_join2data.py) Generate a random combination of titles and viewer counts, or channels\n",
    "# this is a simple version of a congruential generator, \n",
    "#   not a great random generator but enough  \n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "chans   = ['ABC','DEF','CNO','NOX','YES','CAB','BAT','MAN','ZOO','XYZ','BOB']\n",
    "sh1 =['Hot','Almost','Hourly','PostModern','Baked','Dumb','Cold','Surreal','Loud']\n",
    "sh2 =['News','Show','Cooking','Sports','Games','Talking','Talking']\n",
    "vwr =range(17,1053)\n",
    "\n",
    "chvnm=sys.argv[1]  #get number argument, if its n, do numbers not channels,\n",
    "\n",
    "lch=len(chans)\n",
    "lsh1=len(sh1)\n",
    "lsh2=len(sh2)\n",
    "lvwr=len(vwr)\n",
    "ci=1\n",
    "s1=2\n",
    "s2=3\n",
    "vwi=4\n",
    "ri=int(sys.argv[3])\n",
    "for i in range(0,int(sys.argv[2])):  #arg 2 is the number of lines to output\n",
    "\n",
    "    if chvnm=='n':  #no numuber\n",
    "        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],chans[ci]))\n",
    "    else:\n",
    "        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],vwr[vwi])) \n",
    "    ci=(5*ci+ri) % lch   \n",
    "    s1=(4*s1+ri) % lsh1\n",
    "    s2=(3*s1+ri+i) % lsh2\n",
    "    vwi=(2*vwi+ri+i) % lvwr\n",
    " \n",
    "    if (vwi==4): vwi=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_data_join2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python make_join2data.py y 1000 13 > join2_gennumA.txt\n",
    "python make_join2data.py y 2000 17 > join2_gennumB.txt\n",
    "python make_join2data.py y 3000 19 > join2_gennumC.txt\n",
    "python make_join2data.py n 100  23 > join2_genchanA.txt\n",
    "python make_join2data.py n 200  19 > join2_genchanB.txt\n",
    "python make_join2data.py n 300  37 > join2_genchanC.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is exacutly with Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Hadoop streaming command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input3 \\\n",
    "-output /user/cloudera/output_join2 \\   \n",
    "-mapper /home/cloudera/join2_mapper.py \\   \n",
    "-reducer /home/cloudera/join2_reducer.py \\\n",
    "-numReduceTasks 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open ipython for PySpark in Cloudera VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PYSPARK_DRIVER_PYTHON=ipython pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileA = sc.textFile(\"input/join1_FileA.txt\")\n",
    "#Check fileA\n",
    "fileA.collect()\n",
    "# Out[]: [u'able,991', u'about,11', u'burger,15', u'actor,22']\n",
    "fileB = sc.textFile(\"input/join1_FileB.txt\")\n",
    "fileB.collect()\n",
    "'''\n",
    "Out[29]: \n",
    "[u'Jan-01 able,5',\n",
    " u'Feb-02 about,3',\n",
    " u'Mar-03 about,8 ',\n",
    " u'Apr-04 able,13',\n",
    " u'Feb-22 actor,3',\n",
    " u'Feb-23 burger,5',\n",
    " u'Mar-08 burger,2',\n",
    " u'Dec-15 able,100']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper for fileA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_fileA(line):\n",
    "    # split the input line in word and count on the comma\n",
    "    word, count = line.split(',')\n",
    "    # turn the count to an integer  \n",
    "    count = int(count)\n",
    "    return (word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed on running the map transformation to the fileA RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileA_data = fileA.map(split_fileA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mapper is implemented correctly, you should get this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileA_data.collect()\n",
    "#Out[]: [(u'able', 991), (u'about', 11), (u'burger', 15), (u'actor', 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper for fileB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_fileB(line):\n",
    "    # split the input line into word, date and count_string\n",
    "    date, dc = line.split()\n",
    "    word, count_string = dc.split(',')\n",
    "    return (word, date + \" \" + count_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileB_data = fileB.map(split_fileB)\n",
    "fileB_data.collect()\n",
    "'''\n",
    "Out[]: \n",
    "[(u'able', u'Jan-01 5'),\n",
    " (u'about', u'Feb-02 3'),\n",
    " (u'about', u'Mar-03 8 '),\n",
    " (u'able', u'Apr-04 13'),\n",
    " (u'actor', u'Feb-22 3'),\n",
    " (u'burger', u'Feb-23 5'),\n",
    " (u'burger', u'Mar-08 2'),\n",
    " (u'able', u'Dec-15 100')]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to join the two datasets using the words as keys and print for each word the wordcount for a specific date and then the total output from A.\n",
    "\n",
    "Basically for each word in fileB, we would like to print the date and count from fileB but also the total count from fileA.\n",
    "\n",
    "Spark implements the join transformation that given a RDD of (K, V) pairs to be joined with another RDD of (K, W) pairs, returns a dataset that contains (K, (V, W)) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileB_joined_fileA = fileB_data.join(fileA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileB_joined_fileA.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gennum files contain show names and their viewers, genchan files contain show names and their channel. We want to find out the total number of viewer across all shows for the channel BAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gennum files contains show names and number of viewers, you can read into Spark all of them with a pattern matching, see the ? which will match either A, B or C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_views_file = sc.textFile(\"input3/join2_gennum?.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_channel_file = sc.textFile(\"input3/join2_genchan?.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember you can check what Spark is doing by copying some elements of an RDD back to the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_views_file.take(2)\n",
    "# [u'Hourly_Sports,21', u'PostModern_Show,38']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse shows files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next write a function that splits and parses each line of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_show_views(line):\n",
    "    show, views = line.split(',')\n",
    "    return (show, views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use this function to transform the input RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_views = show_views_file.map(split_show_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse channel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to parse each line of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_show_channel(line):\n",
    "    show, channel = line.split(',')\n",
    "    return (show, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to parse the channel files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_channel = show_channel_file.map(split_show_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the 2 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should use the join transformation to join the 2 dataset using the show name as the key.\n",
    "\n",
    "You can join the datasets in any order, as long as you are consistent, both are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_dataset = show_channel.join(show_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract channel as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to find the total viewers by channel, so you need to create an RDD with the channel as key and all the viewer counts, whichever is the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_channel_views(show_views_channel): \n",
    "    cv = show_views_channel[1]\n",
    "    channel = cv[0]\n",
    "    views = int(cv[1])\n",
    "    return (channel, views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can apply this function to the joined dataset to create an RDD of channel and views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_views = joined_dataset.map(extract_channel_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum across all channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to sum all of the viewers for each channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def some_function(a, b):\n",
    "    some_result = a + b\n",
    "    return some_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final stage of your analysis, so you can copy the results back to the Driver with collect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_views.reduceByKey(some_function).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
