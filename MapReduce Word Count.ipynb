{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce for word count with streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will examine the options for streaming that control the number of reducers. In the Cloudera Quickstart VM terminal, I will execute a simple word count example in Python 2.6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper for word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcount_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python   \n",
    "#the above just indicates to use python to intepret this file\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#This mapper code will input a line of text and output <word, 1>\n",
    "# \n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import sys             #a python module with system functions for this OS\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  this 'for loop' will set 'line' to an input line from system \n",
    "#    standard input file\n",
    "# ------------------------------------------------------------\n",
    "for line in sys.stdin:  \n",
    "\n",
    "#-----------------------------------\n",
    "#sys.stdin call 'sys' to read a line from standard input, \n",
    "# note that 'line' is a string object, ie variable, and it has methods that you can apply to it,\n",
    "# as in the next line\n",
    "# ---------------------------------\n",
    "    line = line.strip()  #strip is a method, ie function, associated\n",
    "                         #  with string variable, it will strip \n",
    "                         #   the carriage return (by default)\n",
    "    keys = line.split()  #split line at blanks (by default), \n",
    "                         #   and return a list of keys\n",
    "    for key in keys:     #a for loop through the list of keys\n",
    "        value = 1        \n",
    "        print('{0}\\t{1}'.format(key, value) ) #the {} is replaced by 0th,1st items in format list\n",
    "                            #also, note that the Hadoop default is 'tab' separates key from the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer for word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#This reducer code will input a line of text and \n",
    "#    output <word, total-count>\n",
    "# ---------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "last_key      = None              #initialize these variables\n",
    "running_total = 0\n",
    "\n",
    "# -----------------------------------\n",
    "# Loop thru file\n",
    "#  --------------------------------\n",
    "for input_line in sys.stdin:\n",
    "    input_line = input_line.strip()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Get Next Word    # --------------------------------\n",
    "    this_key, value = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "                          #the split command returns a list of strings, in this case into 2 variables\n",
    "    value = int(value)           #int() will convert a string to integer (this program does no error checking)\n",
    " \n",
    "    # ---------------------------------\n",
    "    # Key Check part\n",
    "    #    if this current key is same \n",
    "    #          as the last one Consolidate\n",
    "    #    otherwise  Emit\n",
    "    # ---------------------------------\n",
    "    if last_key == this_key:       \n",
    "        running_total += value   # add value to running total\n",
    "\n",
    "    else:\n",
    "        if last_key:             #if this key that was just read in\n",
    "                                 #   is different, and the previous \n",
    "                                 #   (ie last) key is not empy,\n",
    "                                 #   then output \n",
    "                                 #   the previous <key running-count>\n",
    "            print( \"{0}\\t{1}\".format(last_key, running_total) )\n",
    "                                 # hadoop expects tab(ie '\\t') \n",
    "                                 #    separation\n",
    "        running_total = value    #reset values\n",
    "        last_key = this_key\n",
    "\n",
    "if last_key == this_key:\n",
    "    print( \"{0}\\t{1}\".format(last_key, running_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the above into a text file as follows from the terminal prompt in Cloudera VM.\n",
    "\n",
    "Type in the following to open a text editor, and then cut and paste the above lines for wordcount_mapper.py into the text editor, save, and exit. Repeat for wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gedit wordcount_mapper.py\n",
    "gedit wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the following to check that the indentations line up as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "more wordcount_mapper.py\n",
    "more wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the following to make it executable. `chmod` means change mode. `chmod +x` <=> `chmod a+x` means make file executable for all users. `+x` means adding executable permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x wordcount_mapper.py\n",
    "chmod +x wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo \"A long time ago in a galaxy far far away\" > /home/cloudera/testfile1\n",
    "echo \"Another episode of Star Wars\" > /home/cloudera/testfile2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`echo` is to write its arguments in the bash to standard output. Syntax: `echo [option(s)] [string(s)] > filename`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory on the HDFS file system (if already exists that’s OK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /user/cloudera/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the files from local filesystem to the HDFS filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -put /home/cloudera/testfile1 /user/cloudera/input\n",
    "\n",
    "hdfs dfs -put /home/cloudera/testfile2 /user/cloudera/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see your files on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/cloudera/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Hadoop WordCount example with the input and output specified.\n",
    "\n",
    "Note that your file paths may differ. The ‘\\’ just means the command continues on next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input \\\n",
    "-output /user/cloudera/output_new \\\n",
    "-mapper /home/cloudera/wordcount_mapper.py \\\n",
    "-reducer /home/cloudera/wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop prints out a whole lot of logging or error information. If it runs you will see something like the following on the screen scroll by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "\n",
    "INFO mapreduce.Job: map 0% reduce 0%\n",
    "\n",
    "INFO mapreduce.Job: map 67% reduce 0%\n",
    "\n",
    "INFO mapreduce.Job: map 100% reduce 0%\n",
    "\n",
    "INFO mapreduce.Job: map 100% reduce 100%\n",
    "\n",
    "INFO mapreduce.Job: Job job_1442937183788_0003 completed successfully\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output file to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/cloudera/output_new/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we have one reducer by default, we can see all the results in one file `part-00000`. However, if we have multiple reducers, the results will be in the multiple files. Then we can check files like `part-00001`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/cloudera/output_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the files there and check out the contents, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/cloudera/output_new/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: `cat` stands for \"catenate\". It reads data from files and outputs their contents. It is the simplest way to dispay the contents of a file at the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s change the number of reduce tasks to see its effects. Setting it to 0 will execute no reducer and only produce the map output. (Note the output directory is changed in the snippet below because Hadoop doesn’t like to overwrite output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input \\\n",
    "-output /user/cloudera/output_new_0 \\\n",
    "-mapper /home/cloudera/wordcount_mapper.py \\\n",
    "-reducer /home/cloudera/wordcount_reducer.py \\\n",
    "-numReduceTasks 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the output file from this run, and then upload it to local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -getmerge /user/cloudera/output_new_0/* wordcount_num0_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number of reducers to 2 and answer the related quiz question at the end of the video lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/cloudera/input \\\n",
    "-output /user/cloudera/output_new_2 \\\n",
    "-mapper /home/cloudera/wordcount_mapper.py \\\n",
    "-reducer /home/cloudera/wordcount_reducer.py \\\n",
    "-numReduceTasks 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results will be stored in two files: `/user/cloudera/output_new_2/part-00000` and `/user/cloudera/output_new_2/part-00001`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
